import torch
import torch.distributed as dist
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams


# Sample sentences
sentences = [
    "A",
    "CCGC",
    "TTGCA",
    "TTGCATTA",
    "GTATTACCGC",
    "TGCCTCCCGTACGT",
    "TAATGTTATATGCCCAGACGGACCCAGACCCGGGTCAGTCCAACTATAAGTACCCCTCGATGCTCCCCAACCCCCCGAATTTTTGTATGCGCGATTTATCATGTCTTCCACCATCCACCCTGTTGCCTGCAACCGCACGTGTGCCGATTACGTCAATCACGCGCGTCACGAGTGCTGCCTAGTTTGCCACCCTCCCTCCCCCACCCCAACTGTTGAGCATTGCCCAACATGTCGGTTGCCCGTCGATCTGTCGGAAGAACTCGCAAAGCTTTCCGCCGCGGTGGTCGAGCTGTCCGATCTCGTCGCCGATCTACACGTTTCTCTCGCCGGCGAAGAACTCGAGGACGCCGAGTCATGAGTACTCGGCGTATTCTCAATGTGACCTCTCGCAAGAAGGTCGACAATATGATGCCCATCGTCGTTGATGAGGAGTCTATTGTTACTGTGGGCCCCTTCACTTCGCCTTCCCCTCTCCTGTGTGTTTTCGTCCCCAACGCTAGGGATACCCGCACCCCCATCACAAACCCTGCCGTTCGGAATTCCTCGGACATCTTTGCAGTCGGTTACCGCGAAAAGGTCCGACTTGATGTCTTAGGTGGTGGAACCTTTATGTGGCGCAGGATTGTTTTCATGCTTAAGGGCGACGATCTTCGGCGCTTCATGGATTCCAGTAATTCTGGCAATATTCCTGCCCAGTTGTTCGATCAGACCACCGAGGGTGGCTGTCGACGTGTCATCGGCCCGCTTTTGGGCGTCACTAACGCCCAGACGGAGCTTCAAAAATATGTCTTCCGTGGTCAGGAAGACGTTGATTGGGCGGATCAATTTACGGCCCCCATCGACACTCGTCGTGTGACCGTTAAGTCCGACAAGATGCGGGTTATTCGGCCCGGTAATGAAACAGGGGCTTCCCGCCTATACCGTTTTTGGTATCCTATCCGCCGCACAATCTCTTACGAGGATGACCTCGAGAGTGATGTCGTCGGTGATCGGCCGTTCTCTACTGCCGGTTTACGGGGGGTGGGAGATATGTACGTCATGGATATTATGGGTATTACGAATTTAACCCCGGATGCACCGCAAACGTCCTACAGGTTCAGCCCTGAGGGCAGTTTTTACTGGCATGAGCGGTAAATTAGGAAACTATGGGGCTATCTAGGTACACGAAGACGCAGTTGGCATTAAGCCAATCTGTGTCAACACCAAGCTCGTCGCGTGGGTCGGAGTTAGATAGCCATATTGAGGGCCGAGCCCAGTGTACCAACTTTTTCCCCTTGTACTTGTCCGTGACATAGAACTGTTTCTGGTGACCTAACCAAAACTTGTAGCTCGGAAGGAACTTTATTCCTCCGAAGTCGTCAAAGATGGCGTATTCAATCCCATCAAGGTCCTCGTCCAGACTAAAGAGGCCTCCGAAGTAAGCATGCTTTCCCAAGCTTCGCGCCCAAACGGTTTTTCCCATTCGGGAAGGCCCGTATACCACGAGTGACTTTCTTCGTTCTGTATATCGAAGTTAGCGGTAATGACACGTAAGGGTCATTGACCCATCAGGGTGGGGTCCCCTATGGGGTGCCCTGTGGGGGCTGTACCCGCGGCAGACACCCCACTATAGGCCCGAGCGTAGCGAAGTTCACTTACCTCCAGTTTGACTTCCGACAAGATTTTCTCGTACCCACTCATCGAGTTCAGCCACCCAAGACGTGTCGATGTTAACTCCCTCCGGAGTCTCATACGGTACTCTGATGGGGGGGAATTTCCAGGCTGCATAAGCTCTGAGTTGGGTGAATGAAGTGACCAATGAACGTGGAGCCAGTGATTGGCATAGCGCCCAAAACTCTGACTCATTCTTTGCATTGATGATTTCAACCCACACCCCACCATTTGCATCCACTCTGCCTCCAGTAGGTCGTTCGAGTCCCCCAGCAACAACGTCTCCATCTTTGATTGCATAATCAAACCCGTCTTCTGGTGTACCACGTGACGGCGATACATTCGGGTGGCATCCTTCAACATCGAATGCACGGGCGTTCCTGGTCCGATATTTGACTCCGAAGTCGACAAAAGCGTGCAAATGAATACCCCCATCTGCGTGATCCTCTCGGCCGATGATGCATTCAGCTCCAAGTCCCGCAAGATGGTCGACCACTGCAAAAGGATCGAGGTCTCCACACTGAGGATAGGTAAGCAAGGCGTACCGTGCTTGAAATCGAAAAGTAGACATAGTTGGTTGTTGCACATTTTGCTTGGGTCCAAGTCTGGGCTTT",
]

def main():
    model_dir= "/expanse/lustre/projects/mia346/swang31/projects/MGFM/MGFM-serving/model_ckpts/safetensors/step-00086000"
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    sampling_params = SamplingParams(logprobs=20)
    llm = LLM(
        model=model_dir,
        task="generate",
        enforce_eager=True,
    )

    for i in range(0, len(sentences), 1):
        batch = sentences[i:i + 1]
        inputs = tokenizer(
            batch,
            padding="max_length",
            truncation=True,
            max_length=512,
            return_tensors="pt")

        output = llm.generate(inputs, sampling_params)
        logprobs_dict = output[0].outputs[0].logprobs[0]
        try:
            rank = list(logprobs_dict.keys()).index(1)
        except ValueError:
            rank = 500
        print(rank)
        print()


if __name__ == "__main__":
    try:
        main()
    finally:
        # Ensure the process group is destroyed upon exiting
        if dist.is_initialized():
            dist.destroy_process_group()
